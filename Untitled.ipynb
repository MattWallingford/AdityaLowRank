{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DistributionNotFound",
     "evalue": "The 'regex!=2019.12.17' distribution was not found and is required by this application, \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/AdityaLowRank/examples/NLU/src/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl-class/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl-class/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl-class/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl-class/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0mrequirers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mto_activate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDistributionNotFound\u001b[0m: The 'regex' distribution was not found and is required by the application",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d030cd35a7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodelingRoberta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/AdityaLowRank/examples/NLU/src/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AdityaLowRank/examples/NLU/src/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AdityaLowRank/examples/NLU/src/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\" require_version wrapper which emits a core-specific hint on failure \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AdityaLowRank/examples/NLU/src/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this application\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# check that the right version is installed if version number was provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDistributionNotFound\u001b[0m: The 'regex!=2019.12.17' distribution was not found and is required by this application, \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"
     ]
    }
   ],
   "source": [
    "from examples.NLU.src.transformers.models import modelingRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'loralib.examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0c8ca30ea124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloralib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'loralib.examples'"
     ]
    }
   ],
   "source": [
    "from loralib.examples.NLU.src.transformers.models.roberta.modeling_roberta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.NLU import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_construct',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_tracing_name',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 100\n",
    "out_features = 100\n",
    "layer = lora.Linear(in_features, out_features, r=16)\n",
    "layer2 = lora.Linear(in_features, out_features, r=16)\n",
    "model = nn.Sequential(layer, layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.annealing_alpha.data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.annealing_alpha\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if  \"annealing_alpha\" in name:\n",
    "        print(name)\n",
    "        param.requires_grad = False\n",
    "        param = torch.tensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_modules(package_name):\n",
    "    modules = []\n",
    "    for _, module_name, _ in pkgutil.walk_packages(path=package_name):\n",
    "        modules.append(module_name)\n",
    "        subpackage = f\"{package_name}.{module_name}\"\n",
    "        modules.extend(find_all_modules(subpackage))\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight: tensor([[-0.0795, -0.0309, -0.0023,  ...,  0.0295, -0.0298, -0.0653],\n",
      "        [-0.0546, -0.0750, -0.0873,  ...,  0.0999,  0.0929,  0.0361],\n",
      "        [-0.0538,  0.0876, -0.0037,  ...,  0.0726,  0.0782, -0.0910],\n",
      "        ...,\n",
      "        [-0.0321, -0.0935, -0.0950,  ..., -0.0923,  0.0907,  0.0724],\n",
      "        [-0.0082, -0.0574,  0.0453,  ...,  0.0799,  0.0624, -0.0266],\n",
      "        [ 0.0750, -0.0494, -0.0203,  ..., -0.0540,  0.0023,  0.0557]])\n",
      "0.bias: tensor([ 0.0725, -0.0791, -0.0930, -0.0318, -0.0483,  0.0055,  0.0681,  0.0431,\n",
      "         0.0728, -0.0236, -0.0719, -0.0211, -0.0707,  0.0848, -0.0710, -0.0904,\n",
      "         0.0268,  0.0750,  0.0224,  0.0583, -0.0820,  0.0459, -0.0910, -0.0544,\n",
      "         0.0862,  0.0693,  0.0055,  0.0471, -0.0091, -0.0154, -0.0441, -0.0517,\n",
      "        -0.0172,  0.0120, -0.0021,  0.0798, -0.0348,  0.0371, -0.0271,  0.0800,\n",
      "        -0.0964, -0.0284,  0.0930,  0.0772, -0.0288,  0.0173,  0.0273,  0.0690,\n",
      "        -0.0587,  0.0243, -0.0605, -0.0245, -0.0873, -0.0460,  0.0006,  0.0354,\n",
      "         0.0127, -0.0027,  0.0670,  0.0449,  0.0763, -0.0816, -0.0552, -0.0341,\n",
      "        -0.0521,  0.0390,  0.0752, -0.0682, -0.0453, -0.0341,  0.0542, -0.0264,\n",
      "         0.0170, -0.0426, -0.0907, -0.0897,  0.0947,  0.0622,  0.0575,  0.0318,\n",
      "        -0.0068,  0.0251,  0.0059,  0.0621,  0.0431,  0.0601, -0.0107,  0.0891,\n",
      "        -0.0920, -0.0183,  0.0264, -0.0664, -0.0273, -0.0542, -0.0556, -0.0506,\n",
      "        -0.0841,  0.0953, -0.0676,  0.0391])\n",
      "0.annealing_alpha: 1.0\n",
      "0.lora_A: tensor([[ 0.0436,  0.0264, -0.0608,  ...,  0.0221,  0.0391,  0.0409],\n",
      "        [ 0.0453,  0.0275, -0.0567,  ..., -0.0694,  0.0826,  0.0937],\n",
      "        [-0.0852, -0.0958, -0.0279,  ..., -0.0809, -0.0054, -0.0437],\n",
      "        ...,\n",
      "        [-0.0744, -0.0683, -0.0108,  ..., -0.0883, -0.0708, -0.0565],\n",
      "        [ 0.0527, -0.0372,  0.0769,  ...,  0.0838,  0.0058, -0.0311],\n",
      "        [-0.0698, -0.0969, -0.0170,  ..., -0.0994,  0.0761,  0.0693]])\n",
      "0.lora_B: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "1.weight: tensor([[ 0.0358, -0.0155, -0.0718,  ...,  0.0366,  0.0703,  0.0275],\n",
      "        [ 0.0370, -0.0619,  0.0161,  ..., -0.0751,  0.0931,  0.0378],\n",
      "        [ 0.0257,  0.0610, -0.0075,  ..., -0.0669, -0.0484, -0.0978],\n",
      "        ...,\n",
      "        [ 0.0898,  0.0648, -0.0043,  ..., -0.0918, -0.0351, -0.0552],\n",
      "        [ 0.0356,  0.0654, -0.0212,  ...,  0.0154, -0.0440, -0.0164],\n",
      "        [-0.0678,  0.0908, -0.0591,  ..., -0.0396,  0.0324,  0.0634]])\n",
      "1.bias: tensor([ 0.0948,  0.0886, -0.0955,  0.0390, -0.0042,  0.0801,  0.0766, -0.0694,\n",
      "        -0.0544,  0.0661, -0.0999,  0.0066,  0.0582, -0.0233,  0.0779,  0.0386,\n",
      "        -0.0354, -0.0892, -0.0647, -0.0602, -0.0036, -0.0102, -0.0684, -0.0634,\n",
      "        -0.0535, -0.0990,  0.0916, -0.0942,  0.0892,  0.0269, -0.0475, -0.0064,\n",
      "        -0.0677,  0.0842,  0.0353,  0.0373,  0.0591,  0.0549, -0.0580,  0.0175,\n",
      "         0.0071, -0.0119, -0.0608, -0.0173,  0.0887,  0.0419,  0.0161, -0.0081,\n",
      "        -0.0649, -0.0951,  0.0174,  0.0517, -0.0220,  0.0777, -0.0308, -0.0672,\n",
      "         0.0548,  0.0741, -0.0175,  0.0352, -0.0287, -0.0229,  0.0843, -0.0735,\n",
      "         0.0734,  0.0946,  0.0223,  0.0843,  0.0286, -0.0802, -0.0691,  0.0226,\n",
      "         0.0454,  0.0544, -0.0535,  0.0791,  0.0634, -0.0923,  0.0654, -0.0869,\n",
      "        -0.0554, -0.0249,  0.0777, -0.0976,  0.0003, -0.0158,  0.0674,  0.0047,\n",
      "         0.0437,  0.0308, -0.0724, -0.0672, -0.0525, -0.0627, -0.0370, -0.0575,\n",
      "        -0.0858,  0.0976,  0.0715, -0.0639])\n",
      "1.lora_A: tensor([[-0.0265,  0.0624, -0.0129,  ...,  0.0544, -0.0164,  0.0269],\n",
      "        [-0.0239, -0.0846,  0.0160,  ...,  0.0772, -0.0157,  0.0808],\n",
      "        [ 0.0803, -0.0309, -0.0806,  ..., -0.0730, -0.0463,  0.0340],\n",
      "        ...,\n",
      "        [ 0.0581, -0.0027,  0.0481,  ..., -0.0673, -0.0431,  0.0192],\n",
      "        [-0.0960, -0.0802,  0.0706,  ..., -0.0662, -0.0858, -0.0081],\n",
      "        [ 0.0327,  0.0681,  0.0417,  ...,  0.0553, -0.0501, -0.0417]])\n",
      "1.lora_B: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
